{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pandas\n",
    "\n",
    "## 1. What is pandas?\n",
    "## Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n",
    "## 2. Why is pandas so popular?\n",
    "## Pandas is the *de facto* standard in data analysis and data manipulation in Python. In the backend it stores data as NumPy arrays, which in turns gives C-like performance whilst mantaining code simplicity.\n",
    "## 3. Any drawback to be aware off?\n",
    "## Pandas is very memory consuming, if a .csv file is X MB, you should expect pandas to take around 5-10 X memory in your RAM. This becomes a problem if you want to load several GB worth of data. Hence, scalability is not ideal and other technologies should be considered to ingest GB or TB sized data streams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Series\n",
    "\n",
    "## Pandas series are the simplest structure available in Pandas. They, describe a dataset labelled by a index and a single column. For instance, this could be a time-series of a stock price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01    101.588572\n",
      "2020-01-02    101.561367\n",
      "2020-01-03    100.171172\n",
      "2020-01-04     98.651032\n",
      "2020-01-05     97.318439\n",
      "                 ...    \n",
      "2020-07-14     99.717189\n",
      "2020-07-15     99.950478\n",
      "2020-07-16     99.864886\n",
      "2020-07-17    100.852199\n",
      "2020-07-18    101.443511\n",
      "Freq: D, Name: stock_price, Length: 200, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# We create a date range\n",
    "dates = pd.date_range('20200101', periods=200)\n",
    "# And some random values for a stock\n",
    "stock_value=100+np.random.normal(0,1,200)\n",
    "\n",
    "df = pd.Series(stock_value, index=dates, name='stock_price')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pandas series offer a number of functionalities, like plotting the data in a very straightforward manner (for more info visit https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.plot.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(title=\"Stock value\",grid=True,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another useful feature is to be able to display summary statistics of our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also display first/last n rows using ```head(n)``` or ```tail(n)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas DataFrames\n",
    "### Before advancing further, let's introduce the concept of a dataframe. A Pandas DataFrame is a collection of Series labelled by the same index, e.g. it could be a collection of Series of Stock prices labelled by their timestamp. Let's have a look at some examples looking at the```yahoo_fin``` library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yahoo_fin.stock_info as si\n",
    "\n",
    "AAPL = si.get_data(\"aapl\") # gets Apple's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(AAPL) # Check data typr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As mentioned, before ```yahoo_fin``` provides us with a dataframe directly with a number of columns, that can then be accessed by typing ```dataframe.columns```, and likewise for the index, which canbe accessed using  ```dataframe.index```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing and loading dataframes\n",
    "### Pandas offers a number of format compatibility like ```dataframe.to_csv``` .csv,  ```dataframe.to_pickle``` .pickle etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.to_csv('AAPL.csv',index=True)\n",
    "pd.read_csv('AAPL.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.to_pickle('AAPL.pickle')\n",
    "pd.read_pickle('AAPL.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing DataFrames\n",
    "\n",
    "### Perhaps, not all the information in a dataframe is relevant to us and often case we want to either remove part of it or transform it. Let us start by selecting a single column from the dataframe above. There are a numbers of ways to do this operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume we are interested in adjclose column in AAPL dataframe\n",
    "AAPL_adjclose=AAPL.adjclose # we can type explicitly the name of the column\n",
    "print(type(AAPL_adjclose))\n",
    "AAPL_adjclose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalently we can use \n",
    "AAPL_adjclose=AAPL['adjclose'] # we can type explicitly the name of the column\n",
    "print(type(AAPL_adjclose))\n",
    "AAPL_adjclose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Remark***:  Slicing a dataframe into a single column returns  a Series!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL['log_stock_close_adj']=np.log(AAPL.adjclose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some useful functions\n",
    "### Pandas provides mean, std,... and a number of functions that can be applied to both axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rolling windows\n",
    "\n",
    "### In time-series it is useful to apply a function on observation windows of size $n$. We can do so using ```dataframe.rolling(n)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let#s compute the 10 day moving average\n",
    "AAPL.adjclose.rolling(10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diff and Shift\n",
    "### ```dataframe.diff(n)``` computes the difference with respect to the tow n postitions below. Shift ``dataframe.shift(n)``` displaces the dataframe $n$ rows down. Using this functions together we can easily compute returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "AAPL['daily_return'] = (AAPL.adjclose.diff(i) - AAPL.adjclose.diff(i - 1)) / AAPL.adjclose.shift(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing using  ```loc ```\n",
    "### Another approach to slice a dataframe is to use ```dataframe.loc(index,columns)```. This allows to retrieve one or more columns. Let's look at some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We try to obtain adjclose column as before\n",
    "AAPL_adjclose=AAPL.loc[:,'adjclose']\n",
    "print(type(AAPL_adjclose))\n",
    "AAPL_adjclose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's slice high and low\n",
    "AAPL_high_low=AAPL.loc[:,['low','high']]\n",
    "print(type(AAPL_high_low))\n",
    "AAPL_high_low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Remark***: When multiple columns are sliced, the returned object is again a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is also posible to slice index and columns at the same time. Let's assume we are only interested in 2020 high and low data. And let's use  ```plot() ``` and  ```describe() ``` in the outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#I strongly recommend to have a look at datetime module as you will be often dealing with this when using time-series data \n",
    "datetime.datetime(2020, 5, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "# We select just 2019 dates\n",
    "dates_2019=AAPL.index[(AAPL.index>=datetime.datetime(2019, 1, 1)) & (AAPL.index<datetime.datetime(2020, 1, 1))]\n",
    "#We slice the dataframe\n",
    "AAPL_high_low=AAPL.loc[dates_2019,['low','high']]\n",
    "print(type(AAPL_high_low))\n",
    "#Plot the data\n",
    "AAPL_high_low.plot(grid=True,title='AAPL 2020 daily high and low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AAPL_high_low.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing using  ```iloc```\n",
    "### in order to use loc one needs to specify the index and column precisely. Using .iloc we can treat the Dataframe as if it was a numpy array and slice using integers as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.iloc[-100:,[0,2,3]]# gives last 100 rows for columns # 0 2 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Remark***: There is also the option to cast a dataframe or series into a numpy array using  ```.values ```. Note that this operation will get rid of the index column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.low.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ```at``` and ```iat```: retrieving single values\n",
    "### When retrieving a single value from a dataframe it is much more efficient to use ```at```/```iat``` instead of ```loc```/```iloc```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit AAPL.iloc[0,0]\n",
    "%timeit AAPL.iat[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boolean slicing\n",
    "\n",
    "### We can also slice a dataframe whenever a boolean condition is satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data by volume\n",
    "AAPL.loc[AAPL.volume<10000000,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carefull with copies!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=AAPL\n",
    "print(AAPL.iloc[0,0])\n",
    "df2.iloc[0,0]=0\n",
    "print(AAPL.iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating over Dataframes\n",
    "### Sometime our analysis requires to go row by row in a dataframe to perform a numerical operation. To do this we can use ```iterrows``` or ```itertuples``` . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "high_low_mid=np.zeros(len(AAPL))\n",
    "index=0\n",
    "start_time=time.time()\n",
    "for row in AAPL.iterrows():\n",
    "    high_low_mid[index]=0.5*(row[1].high+row[1].low)\n",
    "    index+=1\n",
    "print(\"iterrows took\", time.time()-start_time,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(high_low_mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_low_mid=np.zeros(len(AAPL))\n",
    "index=0\n",
    "start_time=time.time()\n",
    "for row in AAPL.itertuples():    \n",
    "    high_low_mid[index]=0.5*(row.high+row.low)\n",
    "    index+=1\n",
    "print(\"itertuples took\", time.time()-start_time,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(high_low_mid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If going row by row is necessary (as we will see in a minute, vectorisation is always preferred), most of the time `itertuples` is much more efficient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorisation in DataFrame Columns and slices\n",
    "### As mentioned in the beginning of the session, internally Pandas stores the data as numpy arrays. Hence, we can make use of vectorisation to speed up computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit high_low_mid=0.5*(AAPL.high+AAPL.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_low_mid=0.5*(AAPL.high+AAPL.low)\n",
    "high_low_mid.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can apply the same pinciple if we want a slice corresponding to 2019 data\n",
    "\n",
    "dates_2019=AAPL.index[(AAPL.index>=datetime.datetime(2019, 1, 1)) & (AAPL.index<datetime.datetime(2020, 1, 1))]\n",
    "\n",
    "%timeit high_low_mid_2019=0.5*(AAPL.high[dates_2019]+AAPL.low[dates_2019])# gives some performance improvement\n",
    "\n",
    "%timeit high_low_mid_2019=0.5*(AAPL.loc[dates_2019,'high']+AAPL.loc[dates_2019,'low'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex functions and  ```apply```\n",
    "### Using ```apply``` we can vectorise any user-defined function that supports vectorisation and apply it indexwise (axis=1) or columnwise (axis=0). One can further optimize the method by setting ```raw=False``` which will make assume objects to be numpy arrays internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL=AAPL.loc[:,['open','high','low','close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnwuse\n",
    "def my_func(x):\n",
    "    # Function will be applied columnwise x represents the entire column\n",
    "    return x.max() -x.min()\n",
    "\n",
    "%timeit AAPL.apply(my_func,axis=0,raw=False)\n",
    "%timeit AAPL.apply(my_func,axis=0,raw=True)\n",
    "\n",
    "AAPL.apply(my_func,axis=0,raw=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func_high_low_mid(x):\n",
    "    \n",
    "     # Function will be applied rowise, x the entire \n",
    "    return 0.5*(x.high+x.low) #here we assume x mantains the column structure\n",
    "\n",
    "def my_func_high_low_mid2(x):\n",
    "    # Function will be applied rowise, x the entire row casted to numpy array\n",
    "    return 0.5*(x[1]+x[2]) # We need to use integer slicing\n",
    "\n",
    "%timeit AAPL.apply(my_func_high_low_mid,axis=1,raw=False)\n",
    "%timeit AAPL.apply(my_func_high_low_mid2,axis=1,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see ```raw=False``` can give dramatic performance improvements as data will be treated as a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join/Merge/Concatenate/Append dataframes and series\n",
    "\n",
    "## 1 Join and Merge\n",
    "\n",
    "### Now that we have a clear view on basic data manipulation, we can ask ourselves how can we merge data from different Stocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_symbols=['aapl','amzn','dis','msft','spy']\n",
    "\n",
    "dict_of_df={}\n",
    "for symbol in list_of_symbols:\n",
    "    dict_of_df[symbol]=si.get_data(symbol) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_df['amzn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_df['aapl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can merge two dataframes by index using ```merge```. By setting ```how='inner'``` we make sure that only intersecting indices will be selected and likewise setting ```left_index=True```, ```right_index=True``` we make sure that both indices are being considered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit merged_df1=dict_of_df['aapl'].merge(dict_of_df['amzn'],how='inner',left_index=True, right_index=True,suffixes=('_aapl','_amzn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df1=dict_of_df['aapl'].merge(dict_of_df['amzn'],how='inner',left_index=True, right_index=True,suffixes=('_aapl','_amzn'))\n",
    "merged_df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likewise we can perform the same operation using  ```join```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit merged_df2=dict_of_df['aapl'].join(dict_of_df['amzn'],how='inner',lsuffix='_aapl',rsuffix='_amzn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2=dict_of_df['aapl'].join(dict_of_df['amzn'],how='inner',lsuffix='_aapl',rsuffix='_amz')\n",
    "merged_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Remark***: the difference between ```merge``` and ```join``` is that ```merge``` allows for more flexibility allowing to merge by column values as well. In general, merge performance tends to be better at the cost of providing more arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Append\n",
    "### ```append``` allows to add additional rows to an existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL.append({\"open\": 100,\"high\": 100,\"low\": 100,\"close\": 100,\"adjclose\": 100 ,\"volume\": 100, \"ticker\": 'AAPL'  },ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_df['aapl'].append(dict_of_df['amzn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Concat and Multi_indexing\n",
    "### Concatenate allows to merge multiple df at once, but will create a multi-index/multi-column data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.concat(dict_of_df,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2=pd.concat(dict_of_df,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This introduces multi_indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[:,('msft','close')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[('aapl'),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby\n",
    "\n",
    "\n",
    "### Groupy allows to index data using different columns or compute summary statistics for groups wihin the data. Let's look at some option data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas\n",
    "aapl = yf.Ticker(\"AMZN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('AMZN_options.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option_data=option_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the syntax is ```dataframe.groupby([columns])```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option_data.groupby([\"expiration_date\",\"option_type\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_option_chain=option_data.groupby([\"expiration_date\",\"option_type\",\"strike\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_option_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_option_chain.loc[('2020-11-06','C'),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove missing data ```dropna``` and ```interpolate```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's introduce some missing values and see how we can remove or interpolate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_copy=AAPL.iloc[:,:-1]\n",
    "AAPL_copy.iloc[2,:]=np.nan\n",
    "AAPL_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropna just removes the rows that contain a NaN\n",
    "AAPL_copy.dropna(axis=0)\n",
    "# We can also use axis=1 to drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_copy.interpolate(method='linear', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further reading for big data\n",
    "\n",
    "### If you are dealing with big data is likely that pandas will consume all your memory, so other tools are preferred which use *lazy evaluation* like VAEX https://pypi.org/project/vaex/ or PySpark https://spark.apache.org/docs/latest/api/python/index.html \n",
    "\n",
    "### Lazy evaluation essentially means that a plan will be set to execute your operation, but it won't be actually executed until you need to retrieve a value. For instance creating a new column can be a lazy operation until we need to retrieve some value in that column.\n",
    "\n",
    "### This approach allows to use multiprocessing to speed up calculations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
